---
title: "wrangle-data"
format: html
editor: visual
---

### Load in data

Here, I load in all the required packages to memory. Then, I read in all the files in a directory and store them in a list.

```{r load data}
#| include: false

library(tidyverse)
library(readxl)
library(sf)
library(ClimateNAr)
library(elevatr)
library(withr)
library(ncdf4)

# define the function to load all .xlsx files in a folder and put them in a list
load_xlsx_files <- function(directory) {
  # get the list of all .xlsx files in the specified directory
  file_list <- list.files(path = directory,
                          pattern = "*.xlsx",
                          full.names = TRUE)
  # extract file names without directory and extension, and clean them
  file_names <- str_replace_all(basename(file_list), 
                                "\\.[xX][lL][sS][xX]$", "")
  cleaned_names <- str_replace_all(file_names, "[^0-9]", "")
  
  # read each file and store the data in a list using purrr::map
  data_list <- purrr::map(file_list, read_excel)
  
  # assign cleaned names to the list elements
  names(data_list) <- cleaned_names
  
  # return the list of data frames
  return(data_list)
}

# run function to load in data
davis <- load_xlsx_files("../01_raw-data/davis/")
stebs <- load_xlsx_files("../01_raw-data/stebs/")

```

### Wrangle data frames

I wrangled the data matrix into observations by date (long format). I also added code to handle some of the idiosyncrasies in the spreadsheet format that caused errors when I tried to read it in.

```{r wrangle data}

# function to fill NA values with the most recent non-NA value in a row
fill_row <- function(row) {
  non_na_value <- NA
  for (i in 1:length(row)) {
    if (!is.na(row[i])) {
      non_na_value <- row[i]
    } else {
      row[i] <- non_na_value
    }
  }
  return(row)
}

# define the wrangle_data function for davis
wrangle_data_davis <- function(df, year) {
  # convert to df
  df <- df %>% as.data.frame()
  
  # remove the first row if there is a running total
  if (!is.na(df[3, 8]) && df[3, 8] == "Adults") {
    new_names <- ifelse(is.na(df[1, ]), paste0("...", seq_along(df[1, ])), df[1, ])
    new_names <- make.names(new_names, unique=TRUE)
    df <- setNames(df, new_names)
    df <- df %>% slice(-1)
  }
  
  # if night/month rows are flip-flopped
  if (df[1, 10] %in% c("N", "D")) {
    df[1, ] <- colnames(df) # colnames to first row
    df <- df %>% # remove day time data
      select(-which(str_detect(df[1, ], "D")))  
    df[1, ] <- df[1, ] %>% # ... to NA
      str_replace(., "\\.\\.\\.", NA_character_) %>% as.list()
  }
    
  # apply the function only to the first row
  df[1, ] <- fill_row(as.vector(df[1, ]))
  
  # make a row for the dates
  date_row <- df %>%
    slice(c(1, 2)) %>%
    mutate_all(~ paste(., collapse = " "))
  
  # add it in
  df[1, ] <- date_row[1, ]
  
  # rename the columns as the first row
  df <- df %>%
    setNames(unlist(df[1, ]))
  
  # remove the last 4 columns
  df <- df[, -c((ncol(df) - 3):ncol(df))]
  
  # remove columns 3 through 6, 8, and 9
  df <- df %>%
    select(-3:-6, -8, -9)
  
  # define new column names for the species columns
  new_names <- c("hodges", "list", "verbatim_name")
  
  # small edits add up :)
  df <- df %>%
    # rename the species columns
    rename_with(~ new_names, c(1, 2, 3)) %>%
    # remove the first 2 rows using dplyr
    slice(-(1:2)) %>%
    # drop columns where "verbatim_name" is NA
    drop_na(verbatim_name) %>%
    # remove extra text
    filter(!verbatim_name == "Species:",
           !verbatim_name == "species (total)",
           !verbatim_name == "Individuals:",
           !verbatim_name == "Off:",
           !verbatim_name == "On:",
           !verbatim_name == "Temp., deg. F.:"
           ) %>%
    # remove periods from names
    rename_with(~ gsub("\\.", "", .), contains(".")) %>%
    # combine the first two columns into a single column
    mutate(hodges_list = ifelse(is.na(list), hodges, paste0(hodges, list))) %>%
    # remove columns that were just used
    select(-c(hodges, list)) %>%
    # put hodges_list first
    select(hodges_list, everything()) %>%
    # pivot dates longer
    pivot_longer(
      cols = -c(hodges_list, verbatim_name),
      names_to = "date",
      values_to = "presence"
    ) %>%
    # remove NA rows (absences)
    mutate(presence = ifelse(is.na(presence), 0, 1)) %>%
    filter(presence != 0) %>%
    # remove presence column
    select(-presence) %>%
    # fix dates
    mutate(date = str_replace(date, "Sept", "Sep")) %>%
    mutate(date = as.Date(paste(date, year), format = "%b %d %Y"))
  
  return(df)
}

# define the wrangle_data function for stebs
wrangle_data_stebs <- function(df, year) {
  # convert to df
  df <- df %>% as.data.frame()
  
  # remove blank columns
df <- df %>%
  select(where(~ {
    if (is.numeric(.) || is.logical(.)) {
      !all(is.na(.) | . == 0 | . == FALSE)
    } else {
      TRUE  # Keep non-numeric and non-logical columns
    }
  }))
    
  # apply the function only to the first row
  df[1, ] <- fill_row(as.vector(df[1, ]))
  
  # make a row for the dates
  date_row <- df %>%
    slice(c(1, 2)) %>%
    mutate_all(~ paste(., collapse = " "))
  
  # add it in
  df[1, ] <- date_row[1, ]
  
  # remove daytime collection columns
  df <- df %>%
    select(-matches("D|NT|Trap|day"))
  
  # delete family column
  if (df[3,3] == "ERIOCRANIIDAE" | df[4,3] == "ERIOCRANIIDAE") {
    df <- df %>%
    select(-3)   
  }
  
  # remove columns 4 and 5
  df <- df %>%
    select(-4:-5)
  
  # remove the last 2 columns
  df <- df[, -c((ncol(df) - 1):ncol(df))]
  
  # rename the columns as the first row
  df <- df %>%
    setNames(unlist(df[1, ]))
  
  # define new column names for the species columns
  new_names <- c("hodges", "list", "verbatim_name")
  
  # small edits add up :)
  df <- df %>%
    # rename the species columns
    rename_with(~ new_names, c(1, 2, 3)) %>%
    # remove the first 2 rows using dplyr
    slice(-(1:2)) %>%
    # drop columns where "verbatim_name" is NA
    drop_na(verbatim_name) %>%
    # remove extra text
    filter(!verbatim_name == "Species:",
           !verbatim_name == "species (total)",
           !verbatim_name == "Individuals:",
           !verbatim_name == "Off:",
           !verbatim_name == "On:",
           !verbatim_name == "Temp., deg. F.:"
           ) %>%
    # remove periods from names
    rename_with(~ gsub("\\.", "", .), contains(".")) %>%
    # combine the first two columns into a single column
    mutate(hodges_list = ifelse(is.na(list), hodges, paste0(hodges, list))) %>%
    # remove columns that were just used
    select(-c(hodges, list)) %>%
    # put hodges_list first
    select(hodges_list, everything()) %>%
    # pivot dates longer
    pivot_longer(
      cols = -c(hodges_list, verbatim_name),
      names_to = "date",
      values_to = "presence"
    ) %>%
    # remove NA rows (absences)
    mutate(presence = ifelse(is.na(presence), 0, 1)) %>%
    filter(presence != 0) %>%
    # remove presence column
    select(-presence) %>%
    # fix dates
    mutate(date = str_replace(date, "Sept", "Sep")) %>%
   mutate(date = as.Date(paste(date, year), format = "%b %d %Y"))
  
  return(df)
}

# wrangle data
davis_wrangled <- imap(davis, wrangle_data_davis) %>% list_rbind()
stebs_wrangled <- imap(stebs, wrangle_data_stebs) %>% list_rbind()

```

### Match names to a backbone

First, I made a list of names, then I exported that list so that I could run a command line script to process the data and tie it back to the GBIF and iNat taxonomy backbones. Then, I created a column based on the sort scores so that I am able to identify which "species" need to be manually checked.

```{r name-match}

# create a function to match names to a backbone
get_names <- function(df, site) {
  # get species names
  taxa <- unique(df$verbatim_name)
  
  # construct file name string
  filename <- paste0("../01_raw-data/", site, "-names/", 
                     deparse(substitute(df)), ".txt")
  
  # construct file output string
  fileoutput <- paste0("../01_raw-data/", site, "-names/", 
                       deparse(substitute(df)), "-output.csv")
  
  # write to a text file for use with names verifier
  writeLines(taxa, filename)
  
  # parse names (prefer iNat (180) and GBIF (11))
  system(paste("gnverifier", filename, ">", fileoutput, " -s '11, 180'"))
  
  # read in matched names
  check_names <- read_csv(fileoutput)
  
  # select columns, rename, and join with obs data
  names_df <- check_names %>%
    select(SortScore, ScientificName, MatchedCanonical, 
           TaxonId, DataSourceTitle) %>%
    rename(sort_score = SortScore,
           verbatim_name = ScientificName,
           genus_species = MatchedCanonical,
           taxon_id = TaxonId,
           source = DataSourceTitle) %>%
    # create a column for bad matches or no matches
    mutate(manual_check = ifelse(sort_score < 9 | str_count(genus_species, "\\S+") < 2 | str_detect(verbatim_name, "\\?"), 1, 0)) %>%
    right_join(., df, join_by(verbatim_name))
  
  return(names_df)
}

# run function on davis names
davis_full <- get_names(davis_wrangled, "davis")
stebs_full <- get_names(stebs_wrangled, "stebs")

```

### Manual names check

This chunk provides exported lists of matched and unmatched names to be gone through by John and Grace.

```{r name-check}

# get list of unmatched names
davis_unmatched <- davis_full %>%
  filter(manual_check == 1) %>%
  distinct(verbatim_name)

# get list of matched names
davis_matched <- davis_full %>%
  filter(manual_check == 0) %>%
  distinct(verbatim_name)

# write unmatched names .csv
write_csv(davis_unmatched, "/Users/gracehorne/Library/CloudStorage/Box-Box/Meineke_Lab/Grace-folder/JAdB-moth-data/01_raw-data/davis-names/davis_unmatched.csv")

# write matched names .csv
write_csv(davis_matched, "/Users/gracehorne/Library/CloudStorage/Box-Box/Meineke_Lab/Grace-folder/JAdB-moth-data/01_raw-data/davis-names/davis_matched.csv")

# get list of unmatched names
stebs_unmatched <- stebs_full %>%
  filter(manual_check == 1) %>%
  distinct(verbatim_name)

# get list of matched names
stebs_matched <- stebs_full %>%
  filter(manual_check == 0) %>%
  distinct(verbatim_name)

# write unmatched names .csv
write_csv(stebs_unmatched, "/Users/gracehorne/Library/CloudStorage/Box-Box/Meineke_Lab/Grace-folder/JAdB-moth-data/01_raw-data/stebs-names/stebs_unmatched.csv")

# write matched names .csv
write_csv(stebs_matched, "/Users/gracehorne/Library/CloudStorage/Box-Box/Meineke_Lab/Grace-folder/JAdB-moth-data/01_raw-data/stebs-names/stebs_matched.csv")

```

### Add checklist number

I joined the Moth Photographer's group checklist numbers to John's data.

```{r pohl-checklist}

checklist <- read_excel("../01_raw-data/MPG-Taxa_20240311.xlsx") %>%
  rename(
    p_no = `P No`,
    genus_species = Genus_Species,
    common_name = `Common Name`,
    family = Family,
    genus = Genus,
    species = Species
  ) %>%
select(p_no, MONA, genus_species, common_name, family, genus, species)


davis_full <- left_join(davis_full, checklist)
stebs_full <- left_join(stebs_full, checklist)

```

### ⚠️ Add John's confidence levels

```{r john-species-confidence}

```

### ClimateNA data

Here, I downloaded the data from ClimateNA programmatically. Because of the limits on queries one can ask of the database, I've opted to go with Terraclimate instead (below), for which no such limitations exist.

```{r climateNA}
#| eval: false

# function to get elevation from a latitude and longitude
get_elevation <- function(site, lon, lat) {
  # create a single point
  point <- st_point(c(lon, lat))
  point_sf <- st_sfc(point, crs = 4326)
  point_sf <- st_sf(geometry = point_sf)
  
  # get elevation
  eldat <- get_elev_point(point_sf)
  
  # add columns for lat and lon and site plus rename elevation column
  eldat$lon <- lon
  eldat$lat <- lat
  eldat$ID1 <- site
  eldat$ID2 <- site
  eldat$el <- eldat$elevation
  
  return(eldat)
}

# get data for davis and stebs
davis_el <- get_elevation(site = "davis", lat = 38.5569, lon = -121.78)
stebs_el <- get_elevation(site = "stebs", lat = 38.51, lon = -122.0972)

# bind dataframes together
site_info <- rbind(davis_el, stebs_el) %>% tibble() %>%
  select(-elevation, -geometry, -elev_units) %>%
  select(ID1, ID2, lat, lon, el)

test <- ClimateNAr::ClimateNA_API2(
  ClimateBC_NA = "NA",
  inputFile = site_info,
  period = "Year_2022.ann",
  MSY = "M"
)

# empty list to be populated
climate_data <- list()

# year range
years <- 1989:2023

# Break the sequence into chunks of 5
chunks <- split(years, ceiling(seq_along(years) / 5))

# get climate data for stebs and davis for year range
for (year in chunks[[1]]) {
  climate_data[[paste(year)]] <- ClimateNAr::ClimateNA_API2(
  ClimateBC_NA = "NA",
  inputFile = site_info,
  period = paste0("Year_", year, ".ann"),
  MSY = "M"
)
}

```

### Terraclimate data

I scraped the data from terraclimate for Davis and Cold Canyon locations between 1989 and 2023. I then put the data is a list called `dat` with sublists for Davis and Cold Canyon.

```{r terraclimate}

# make a df with dates
d1 <- as.Date("19580101", "%Y%m%d") # start date
d2 <- as.Date("20231231", "%Y%m%d") # end date

# fill in dates between
dat <- format(seq(d1, d2, by = "month"), "%m %Y") %>% 
  as_tibble() %>%
  separate(value, into = c("month", "year"), sep = " ")

# list for davis and stebs with all dates
dat <- list("davis" = dat, "stebs" = dat)

# for loop for both sites to get data
for (site in c("davis", "stebs")) {
  
  temp_dat = tibble(.rows = 792) # must equal number of months in dataset
  
  if (site == "davis") {
    x = c(-121.78, 38.5569) # Davis long, lat
  } else if (site == "stebs") {
    x = c(-122.0972, 38.51) # Stebs long, lat
  }
  
  # gather all variables of interest
for (var in c("tmax", "tmin", "ppt")) {
  baseurlagg <-
    paste0(
      paste0(
        "http://thredds.northwestknowledge.net:8080/thredds/dodsC/agg_terraclimate_",
        var
      ),
      "_1958_CurrentYear_GLOBE.nc"
    )
  
  nc <- nc_open(baseurlagg)
  lon <- ncvar_get(nc, "lon")
  lat <- ncvar_get(nc, "lat")
  flat = match(abs(lat - x[2]) < 1 / 48, 1)
  latindex = which(flat %in% 1)
  flon = match(abs(lon - x[1]) < 1 / 48, 1)
  lonindex = which(flon %in% 1)
  start <- c(lonindex, latindex, 1)
  count <- c(1, 1, -1)
  
  # read in the full period of record using aggregated files
  data = as.numeric(ncvar_get(nc, varid = var, start = start, count)) %>%
    as_tibble_col(., column_name = var)
  temp_dat = temp_dat %>% add_column(data)
  
}
  temp_dat = temp_dat %>% mutate(dataset = site)
  dat[[site]] = cbind(temp_dat, dat[[site]])
}

TerraClimate <- dat %>% map_dfr(~ as_tibble(.)) %>% filter(year >= 1989 & year < 2024) %>%
  mutate(source = "terraclimate",
         month = as.numeric(month),
         year = as.numeric(year))

```

### ⚠️ Format seasonal weather data

```{r seasonal-weathee}

```

### ⚠️ Calculate adjusted FDPs

```{r fdps}

```

### ⚠️ Add in pest, fire, and trail data

```{r predictors}

```
